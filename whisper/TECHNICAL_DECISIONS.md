# 🛡️ 反诈 AI 电话专员 - 技术决策文档

## 📋 文档概述

本文档记录了项目开发过程中的重要技术决策、性能分析和架构选择，为后续优化和部署提供参考。

---

## 🔄 双工通信技术分析

### 真双工 vs 伪双工对比

| 特性           | 真双工 (WebSocket)  | 伪双工 (HTTP 流式 + 本地打断) |
| -------------- | ------------------- | ----------------------------- |
| **连接方式**   | 持续 WebSocket 连接 | HTTP 流式请求 + 本地 VAD      |
| **延迟表现**   | 连接建立慢(>2s)     | 连接快速(<100ms)              |
| **打断机制**   | 服务端实时检测      | 本地 VAD 检测 + 0.5s 延迟     |
| **网络开销**   | 持续连接占用        | 按需连接                      |
| **实现复杂度** | 高(连接管理、重连)  | 中(HTTP 重试)                 |
| **稳定性**     | 需要复杂错误处理    | HTTP 标准错误处理             |

### 技术决策：采用伪双工方案

**原因：**

1. **网络环境限制**：WebSocket 连接建立时间>2 秒，严重影响用户体验
2. **实用性优先**：本地 VAD + 0.5 秒延迟已能提供良好的打断体验
3. **稳定性考虑**：HTTP 协议更成熟，错误处理更简单
4. **部署便利性**：无需特殊服务端配置

**结论：** 在当前网络 API 环境下，伪双工方案是最优选择

---

## 🎭 部分模式 (Partial Mode) 分析

### 功能说明

部分模式允许预设 AI 回复的开头内容，适用于：

- 客服场景：强制以"亲爱的客户，您好"开头
- 紧急场景：强制以"请立即停止操作！"开头
- 角色扮演：强制以特定身份回复

### 在反诈场景中的应用潜力

| 场景         | 不使用部分模式            | 使用部分模式              | 效果提升  |
| ------------ | ------------------------- | ------------------------- | --------- |
| **开场白**   | "您好，这里是反诈中心..." | "您好，这里是反诈中心..." | ❌ 无意义 |
| **紧急警告** | AI 自由发挥               | "请立即停止操作！..."     | ✅ 更直接 |
| **安抚用户** | AI 自由发挥               | "请您不要担心，..."       | ✅ 更温和 |
| **专业解释** | AI 自由发挥               | "作为反诈专员，..."       | ✅ 更权威 |

### 技术决策：API 环境下不启用部分模式

**关键问题：双重 API 请求延迟**

```
传统流程：用户输入 → API请求 → 流式回复 → TTS播放
部分模式：用户输入 → 场景检测API → 部分模式API → 流式回复 → TTS播放
```

**性能影响分析：**

- **额外延迟**：+1-2 秒（额外的 API 请求）
- **成本增加**：每次对话增加 1 次 API 调用
- **复杂度提升**：需要场景检测逻辑
- **收益有限**：在短对话场景下提升不明显

**决策原因：**

1. **延迟优先**：反诈场景要求快速响应，额外 1-2 秒延迟不可接受
2. **成本考虑**：API 调用成本翻倍
3. **实用性**：当前 prompt 已能产生专业的反诈回复
4. **简化架构**：减少系统复杂度

**适用场景：**

- ✅ **本地 GPU 部署**：延迟可控，可考虑启用
- ✅ **长对话场景**：多轮对话中的收益更明显
- ❌ **API 环境**：延迟和成本不可接受

---

## 💾 上下文缓存功能

### 功能实现

已为 HTTP 和 WebSocket 版本都实现了上下文缓存：

```python
# 缓存系统提示词，减少重复传输
cache_data = {
    "model": "kimi-k2-turbo-preview",
    "messages": [{"role": "system", "content": PERSONA_PROMPT_V2}],
    "ttl": 3600,  # 1小时
    "tags": ["antifraud_system_prompt"]
}
```

### 性能优化效果

| 指标               | 无缓存       | 有缓存         | 改善           |
| ------------------ | ------------ | -------------- | -------------- |
| **系统提示词传输** | 每次完整发送 | 仅发送缓存引用 | 减少 90%数据量 |
| **Token 消耗**     | 完整计费     | 缓存部分免费   | 节省 30-50%    |
| **网络延迟**       | 完整传输时间 | 引用传输时间   | 减少 20-30ms   |

### 成本节省计算

以系统提示词 1000 tokens 为例：

- **无缓存**：每次对话消耗 1000 tokens
- **有缓存**：首次 1000 tokens + 后续仅引用
- **节省效果**：10 轮对话节省 9000 tokens ≈ 节省 90%成本

---

## 🔧 多轮对话和上下文管理

### 实现特性

1. **智能上下文截断**：

   - 默认保留最新 20 条对话
   - 系统消息始终保留
   - 自动清理过期对话

2. **对话状态管理**：

   ```python
   # 对话统计
   ai.get_conversation_summary()
   # 输出：对话历史: 15条消息 (用户: 8, AI: 7)

   # 清空历史
   ai.clear_conversation_history()
   ```

3. **内存优化**：
   - 避免无限增长的对话历史
   - 保持 API 请求大小可控
   - 防止超出模型上下文窗口
   - **反诈场景优化**：上下文长度设为 50 条（考虑复杂诈骗案例需要更多轮对话）

---

## 🔄 重试机制

### 实现策略

```python
# 统一重试配置
MAX_ATTEMPTS = 100
RETRY_DELAY = 1  # 秒

# 指数退避可选实现
for attempt in range(max_attempts):
    try:
        # API调用
        return success_result
    except Exception as e:
        if attempt < max_attempts - 1:
            time.sleep(RETRY_DELAY)
        else:
            return error_result
```

### 适用场景

- ✅ **网络波动**：临时连接问题
- ✅ **API 限流**：短期超出限制
- ✅ **服务端异常**：临时服务不可用
- ❌ **认证错误**：API 密钥问题（不应重试）
- ❌ **参数错误**：请求格式问题（不应重试）

---

## 📊 性能测试重点

### 关键指标定义

1. **首 Token 延迟**：API 响应第一个字符的时间
2. **首次音频播报延迟**：从用户输入到开始播放语音的时间 ⭐
3. **总处理时间**：完整对话轮次的耗时

### 测试重点：首次音频播报延迟

**为什么这是最重要的指标？**

- 用户感知的真实响应时间
- 包含完整的处理链路：API → TTS → 音频播放
- 直接影响对话流畅度

**测试方法：**

```python
request_start = time.time()
# ... API调用和TTS处理
while not tts.is_playing:
    time.sleep(0.001)
first_audio_delay = time.time() - request_start
```

---

## 🔍 WebSocket vs HTTP 实测分析

### 性能测试结果

基于实际测试数据的分析：

| 指标                     | HTTP 流式 | WebSocket 流式 | 差异                |
| ------------------------ | --------- | -------------- | ------------------- |
| **平均首 token 延迟**    | 1.062s    | ~0.973s        | WebSocket 快约 90ms |
| **平均首次音频播报延迟** | 1.436s    | ~1.4s          | 基本相当            |
| **总处理时间**           | 15.541s   | ~15s           | 基本相当            |

### 关键发现：WebSocket 优势被 TTS 架构抵消

**问题根源：**

```
WebSocket流程：用户输入 → 快速首token(0.97s) → 等待完整回复 → TTS处理 → 音频播放
HTTP流程：   用户输入 → 首token(1.06s) → 等待完整回复 → TTS处理 → 音频播放
```

**核心问题：**

- WebSocket 确实更快收到首 token（快约 90ms）
- 但当前 TTS 需要**完整文本**才能开始合成
- 用户感知的"首次音频播报延迟"几乎相同
- WebSocket 的优势在 TTS 批处理阶段被完全抵消

### 技术决策：流式 TTS 的性能 vs 质量权衡

**实测数据：**

- HTTP 首次音频延迟：1.477s
- WebSocket 首次音频延迟：1.257s
- **WebSocket 优势：220ms (约 15%提升)**

**质量代价：**

- 流式 TTS 缺少完整句子上下文
- 语气和语调可能略显不自然
- 句子间的连贯性下降

**结论：**
WebSocket 确实有性能优势，但需要权衡语音质量。在反诈场景下，**快速响应比完美语调更重要**。

## ⚖️ 流式 TTS 权衡分析

### 性能收益 vs 质量代价

| 方面           | 完整句子 TTS | 流式 TTS | 权衡评估      |
| -------------- | ------------ | -------- | ------------- |
| **响应速度**   | 较慢         | 快 220ms | ✅ 显著提升   |
| **语音质量**   | 自然流畅     | 略显生硬 | ⚠️ 可接受降级 |
| **语气连贯性** | 完整         | 片段化   | ⚠️ 轻微影响   |
| **用户体验**   | 等待时间长   | 即时反馈 | ✅ 明显改善   |

### 反诈场景特殊考虑

**为什么选择性能优先：**

1. **紧急性**：诈骗预警需要快速打断用户操作
2. **注意力**：220ms 的提升能显著提高用户注意力
3. **权威感**：快速响应增强 AI 专员的专业感
4. **实用性**：轻微的语调不自然不影响信息传达

**质量影响评估：**

- 🟢 **可接受**：信息传达清晰，不影响理解
- 🟡 **轻微**：语调略显机械，但在可接受范围
- 🔴 **不影响**：反诈内容以信息准确性为主

### 最终技术选型建议

**推荐方案：WebSocket + 流式 TTS**

**理由：**

1. 220ms 的性能提升在反诈场景下价值很高
2. 语音质量降级在可接受范围内
3. 用户更关心响应速度而非完美语调
4. 技术复杂度增加有限，收益明显

---

## 🔮 后续技术规划

### 短期优化 (1-3 个月)

1. **TTS 流式优化** ⭐ **最重要**

   - 实现句子级流式 TTS（遇到句号立即开始合成）
   - 这将让 WebSocket 的首 token 优势真正体现
   - 预期可将首次音频播报延迟减少 200-500ms
   - 技术方案：检测句子结束符（。！？）就立即开始 TTS

2. **HTTP vs WebSocket 最终对比**

   - 在 TTS 流式优化后重新测试
   - 预期 WebSocket 将显示明显优势
   - 基于新数据做最终技术选型

3. **缓存策略优化**
   - 动态 TTL 调整
   - 多级缓存策略
   - 缓存命中率监控

### 中期规划 (3-6 个月)

1. **本地 GPU 部署准备**

   - 模型选型和性能测试
   - 部分模式重新评估
   - 真双工 WebSocket 实现

2. **智能场景检测**
   - 基于关键词的场景识别
   - 动态调整回复策略
   - A/B 测试不同策略效果

### 长期愿景 (6-12 个月)

1. **多模态融合**

   - 语音情感分析
   - 实时风险评估
   - 多模型并行处理

2. **边缘计算部署**
   - 本地推理优化
   - 混合云边架构
   - 离线模式支持

---

## 📝 技术决策总结

| 技术选择              | 决策                             | 主要原因                         |
| --------------------- | -------------------------------- | -------------------------------- |
| **双工通信**          | 伪双工 (HTTP 流式 + 本地打断)    | 网络环境限制，稳定性优先         |
| **HTTP vs WebSocket** | WebSocket 快 220ms，但有质量代价 | 性能 vs 质量权衡，反诈场景选性能 |
| **部分模式**          | API 环境下禁用                   | 避免双重请求延迟                 |
| **上下文缓存**        | 启用                             | 显著节省成本和延迟               |
| **重试机制**          | 启用 (100 次，1 秒间隔)          | 提高系统稳定性                   |
| **上下文管理**        | 智能截断 (50 条)                 | 适应反诈长对话场景               |
| **TTS 优化**          | 计划句子级流式处理               | 让 WebSocket 优势真正体现        |

---

## 🎯 关键性能目标

- **首次音频播报延迟** < 2 秒 (目标 < 1.5 秒)
- **API 成功率** > 99%
- **对话连续性** 支持 20 轮以上
- **成本控制** 相比无优化版本节省 30%以上

---

_最后更新：2025-01-12_
_下次评估：本地 GPU 部署后重新评估部分模式和真双工的价值_
