# æ–‡ä»¶: brain_ai_module.py

from openai import OpenAI
from typing import Generator, Optional, Dict, Any
import random
import time
import httpx
import json
from . import config
from .prompts import (
    PERSONA_PROMPT_V2,
    OPENING_PROMPT_TEMPLATE,
    WHISPER_PROMPT,
    ERROR_RESPONSES,
    RANDOM_ELEMENTS
)

class KimiAI:
    """å°è£…Kimiå¤§æ¨¡å‹è°ƒç”¨ï¼Œç®¡ç†å¤šè½®å¯¹è¯å†å²ã€‚"""

    def __init__(self, max_context_messages: int = 50, use_cache: bool = True):
        try:
            self.client = OpenAI(
                api_key=config.MOONSHOT_API_KEY,
                base_url="https://api.moonshot.cn/v1",
            )
            self.conversation_history = []
            self.max_context_messages = max_context_messages
            self.system_messages = [{"role": "system", "content": PERSONA_PROMPT_V2}]

            # ç¼“å­˜ç›¸å…³
            self.use_cache = use_cache
            self.cache_tag = "antifraud_system_prompt"
            self.cache_ttl = 3600  # 1å°æ—¶
            self.cached_system_ready = False

            if self.use_cache:
                self._setup_system_cache()

            print(f"Kimi AIå¤§è„‘åˆå§‹åŒ–æˆåŠŸ (V2å£è¯­ä¼˜åŒ–ç‰ˆï¼Œä¸Šä¸‹æ–‡é™åˆ¶: {max_context_messages}æ¡ï¼Œç¼“å­˜: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'})ã€‚")
        except Exception as e:
            print(f"Kimi AIåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _chat_with_retry(self, messages: list, max_attempts: int = 100) -> Generator[str, None, None]:
        """å¸¦é‡è¯•æœºåˆ¶çš„èŠå¤©å®Œæˆ"""
        import time

        st_time = time.time()

        for attempt in range(max_attempts):
            print(f"HTTPå°è¯•: {attempt+1}/{max_attempts}")
            try:
                response = self.client.chat.completions.create(
                    model=config.KIMI_MODEL_NAME,
                    messages=messages,
                    temperature=config.KIMI_TEMPERATURE,
                    max_tokens=config.KIMI_MAX_TOKENS,
                    stream=True
                )

                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                ed_time = time.time()
                print(f"HTTPæŸ¥è¯¢æˆåŠŸ! è€—æ—¶: {ed_time-st_time:.2f}ç§’")
                return

            except Exception as e:
                print(f"HTTPå°è¯• {attempt+1} å¤±è´¥: {e}")
                if attempt < max_attempts - 1:
                    print("1ç§’åé‡è¯•...")
                    time.sleep(1)
                else:
                    print("HTTPæŸ¥è¯¢å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    yield ERROR_RESPONSES["api_error"]
                    return

    def _setup_system_cache(self):
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯ç¼“å­˜"""
        try:
            print("ğŸ—„ï¸ æ­£åœ¨è®¾ç½®ç³»ç»Ÿæç¤ºè¯ç¼“å­˜...")

            headers = {
                "Authorization": f"Bearer {config.MOONSHOT_API_KEY}",
                "Content-Type": "application/json"
            }

            cache_data = {
                "model": "moonshot-v1-128k",  # ç¼“å­˜APIä½¿ç”¨çš„æ¨¡å‹åç§°
                "messages": self.system_messages,
                "ttl": self.cache_ttl,
                "tags": [self.cache_tag]
            }

            response = httpx.post(
                f"{self.client.base_url}caching",
                headers=headers,
                json=cache_data,
                timeout=30.0
            )

            if response.status_code == 200:
                self.cached_system_ready = True
                print(f"âœ… ç³»ç»Ÿæç¤ºè¯ç¼“å­˜è®¾ç½®æˆåŠŸ (æ ‡ç­¾: {self.cache_tag}, TTL: {self.cache_ttl}s)")
            else:
                print(f"âš ï¸ ç¼“å­˜è®¾ç½®å¤±è´¥: {response.status_code} - {response.text}")
                self.use_cache = False

        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜è®¾ç½®å¼‚å¸¸: {e}")
            self.use_cache = False

    def _get_cached_messages(self, user_messages: list) -> list:
        """è·å–ä½¿ç”¨ç¼“å­˜çš„æ¶ˆæ¯åˆ—è¡¨"""
        if self.use_cache and self.cached_system_ready:
            # ä½¿ç”¨ç¼“å­˜å¼•ç”¨æ›¿ä»£ç³»ç»Ÿæ¶ˆæ¯
            cached_messages = [
                {
                    "role": "cache",
                    "content": f"tag={self.cache_tag};reset_ttl={self.cache_ttl}"
                }
            ]
            cached_messages.extend(user_messages)
            return cached_messages
        else:
            # ä¸ä½¿ç”¨ç¼“å­˜ï¼ŒåŒ…å«å®Œæ•´ç³»ç»Ÿæ¶ˆæ¯
            full_messages = []
            full_messages.extend(self.system_messages)
            full_messages.extend(user_messages)
            return full_messages

    def _refresh_cache_if_needed(self):
        """æ ¹æ®éœ€è¦åˆ·æ–°ç¼“å­˜"""
        if self.use_cache and not self.cached_system_ready:
            print("ğŸ”„ å°è¯•é‡æ–°è®¾ç½®ç¼“å­˜...")
            self._setup_system_cache()

    def _make_messages(self, user_input: str = None) -> list:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ”¯æŒç¼“å­˜"""
        # å¦‚æœæœ‰ç”¨æˆ·è¾“å…¥ï¼Œæ·»åŠ åˆ°å†å²
        if user_input:
            self.conversation_history.append({"role": "user", "content": user_input})

        # æ§åˆ¶å†å²æ¶ˆæ¯é•¿åº¦
        if len(self.conversation_history) > self.max_context_messages:
            # åªä¿ç•™æœ€æ–°çš„æ¶ˆæ¯
            self.conversation_history = self.conversation_history[-self.max_context_messages:]
            print(f"ğŸ“ ä¸Šä¸‹æ–‡å·²æˆªæ–­ï¼Œä¿ç•™æœ€æ–° {self.max_context_messages} æ¡æ¶ˆæ¯")

        # ä½¿ç”¨ç¼“å­˜æ„å»ºæ¶ˆæ¯
        return self._get_cached_messages(self.conversation_history)

    def _chat_with_partial_mode(self, messages: list, partial_content: str = "",
                               partial_name: str = "", max_attempts: int = 100) -> Generator[str, None, None]:
        """æ”¯æŒéƒ¨åˆ†æ¨¡å¼çš„èŠå¤©å®Œæˆ"""
        import time

        # å¦‚æœæœ‰éƒ¨åˆ†å†…å®¹ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯æœ«å°¾
        if partial_content or partial_name:
            partial_message = {
                "role": "assistant",
                "partial": True,
                "content": partial_content
            }
            if partial_name:
                partial_message["name"] = partial_name

            messages.append(partial_message)
            print(f"ğŸ­ å¯ç”¨éƒ¨åˆ†æ¨¡å¼: name='{partial_name}', content='{partial_content}'")

        st_time = time.time()

        for attempt in range(max_attempts):
            print(f"HTTPå°è¯•: {attempt+1}/{max_attempts}")
            try:
                response = self.client.chat.completions.create(
                    model=config.KIMI_MODEL_NAME,
                    messages=messages,
                    temperature=config.KIMI_TEMPERATURE,
                    max_tokens=config.KIMI_MAX_TOKENS,
                    stream=True
                )

                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield content

                ed_time = time.time()
                print(f"HTTPæŸ¥è¯¢æˆåŠŸ! è€—æ—¶: {ed_time-st_time:.2f}ç§’")

                # å¦‚æœä½¿ç”¨äº†éƒ¨åˆ†æ¨¡å¼ï¼Œéœ€è¦åˆå¹¶å®Œæ•´å†…å®¹
                if partial_content:
                    full_response = partial_content + full_response

                return full_response

            except Exception as e:
                print(f"HTTPå°è¯• {attempt+1} å¤±è´¥: {e}")
                if attempt < max_attempts - 1:
                    print("1ç§’åé‡è¯•...")
                    time.sleep(1)
                else:
                    print("HTTPæŸ¥è¯¢å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    yield ERROR_RESPONSES["api_error"]
                    return ""

    def generate_opening_statement(self) -> Generator[str, None, None]:
        """ç”ŸæˆåŠ¨æ€å¼€åœºç™½"""
        try:
            # ç”Ÿæˆéšæœºå…ƒç´ 
            carrier = random.choice(RANDOM_ELEMENTS["carriers"])
            hour = random.choice(RANDOM_ELEMENTS["hours"])
            country_code = random.choice(RANDOM_ELEMENTS["country_codes"])
            last_digits = random.randint(*RANDOM_ELEMENTS["digit_range"])
            
            # æ„å»ºå¼€åœºç™½æç¤º
            opening_prompt = OPENING_PROMPT_TEMPLATE.format(
                carrier=carrier,
                hour=hour,
                country_code=country_code,
                last_digits=last_digits
            )
            
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†æ„å»ºæ¶ˆæ¯
            messages = self._make_messages(opening_prompt)

            # æ”¶é›†å®Œæ•´å›å¤ç”¨äºå†å²è®°å½•
            full_response = ""
            for chunk in self._chat_with_retry(messages):
                full_response += chunk
                yield chunk

            # æ·»åŠ åˆ°å¯¹è¯å†å²
            if full_response:
                self.conversation_history.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            print(f"å¼€åœºç™½ç”Ÿæˆå¤±è´¥: {e}")
            yield ERROR_RESPONSES["opening_error"]

    def _detect_urgent_scenario(self, user_input: str) -> Dict[str, Any]:
        """æ£€æµ‹ç´§æ€¥åœºæ™¯ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨éƒ¨åˆ†æ¨¡å¼"""
        urgent_keywords = ["è½¬è´¦", "æ±‡æ¬¾", "éªŒè¯ç ", "é“¶è¡Œå¡", "å¯†ç ", "èº«ä»½è¯", "ä¸‹è½½", "APP", "é“¾æ¥", "æ‰«ç "]
        comfort_keywords = ["å®³æ€•", "æ‹…å¿ƒ", "ç´§å¼ ", "ä¸çŸ¥é“", "æ€ä¹ˆåŠ", "æ…Œ"]

        # æ£€æµ‹é«˜å±æ“ä½œ
        if any(keyword in user_input for keyword in urgent_keywords):
            return {
                "use_partial": True,
                "partial_content": "è¯·ç«‹å³åœæ­¢æ“ä½œï¼",
                "reason": "æ£€æµ‹åˆ°é«˜å±æ“ä½œå…³é”®è¯"
            }

        # æ£€æµ‹éœ€è¦å®‰æŠšçš„æƒ…ç»ª
        if any(keyword in user_input for keyword in comfort_keywords):
            return {
                "use_partial": True,
                "partial_content": "è¯·æ‚¨ä¸è¦æ‹…å¿ƒï¼Œ",
                "reason": "æ£€æµ‹åˆ°éœ€è¦å®‰æŠšçš„æƒ…ç»ª"
            }

        # é»˜è®¤ä½¿ç”¨ä¸“ä¸šèº«ä»½
        return {
            "use_partial": True,
            "partial_name": "åè¯ˆä¸“å‘˜",
            "partial_content": "",
            "reason": "ä½¿ç”¨ä¸“ä¸šèº«ä»½"
        }

    def get_response_stream(self, user_input: str, use_partial_mode: bool = False,
                           partial_content: str = "", partial_name: str = "") -> Generator[str, None, None]:
        """è·å–AIå›å¤çš„æµå¼å“åº”"""
        try:
            # åˆ·æ–°ç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            self._refresh_cache_if_needed()

            # APIç¯å¢ƒä¸‹é»˜è®¤ä¸ä½¿ç”¨éƒ¨åˆ†æ¨¡å¼ï¼ˆé¿å…åŒé‡è¯·æ±‚å»¶è¿Ÿï¼‰
            if use_partial_mode:
                print("âš ï¸ æ³¨æ„ï¼šéƒ¨åˆ†æ¨¡å¼ä¼šå¢åŠ APIè¯·æ±‚å»¶è¿Ÿï¼Œå»ºè®®ä»…åœ¨æœ¬åœ°GPUç¯å¢ƒä½¿ç”¨")

            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†æ„å»ºæ¶ˆæ¯
            messages = self._make_messages(user_input)

            # æ”¶é›†å®Œæ•´å›å¤
            full_response = ""

            if use_partial_mode:
                for chunk in self._chat_with_partial_mode(messages, partial_content, partial_name):
                    full_response += chunk
                    yield chunk
            else:
                for chunk in self._chat_with_retry(messages):
                    full_response += chunk
                    yield chunk

            # æ·»åŠ AIå›å¤åˆ°å†å²
            if full_response:
                self.conversation_history.append({"role": "assistant", "content": full_response})

        except Exception as e:
            print(f"HTTP AIå›å¤ç”Ÿæˆå¤±è´¥: {e}")
            yield ERROR_RESPONSES["api_error"]

    def get_conversation_summary(self) -> str:
        """è·å–å¯¹è¯æ‘˜è¦"""
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯å†å²"

        total_messages = len(self.conversation_history)
        user_messages = len([msg for msg in self.conversation_history if msg["role"] == "user"])
        assistant_messages = len([msg for msg in self.conversation_history if msg["role"] == "assistant"])

        return f"å¯¹è¯å†å²: {total_messages}æ¡æ¶ˆæ¯ (ç”¨æˆ·: {user_messages}, AI: {assistant_messages})"

    def clear_conversation_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        print("ğŸ“ å¯¹è¯å†å²å·²æ¸…ç©º")

class BrainAIModule:
    """AIå¤§è„‘æ¨¡å—ï¼Œè´Ÿè´£ç”Ÿæˆæ™ºèƒ½å›å¤"""
    
    def __init__(self):
        self.kimi = KimiAI()
    
    def generate_opening_statement(self) -> Generator[str, None, None]:
        """ç”Ÿæˆå¼€åœºç™½"""
        return self.kimi.generate_opening_statement()
    
    def get_response_stream(self, user_input: str) -> Generator[str, None, None]:
        """è·å–å¯¹ç”¨æˆ·è¾“å…¥çš„å›å¤"""
        return self.kimi.get_response_stream(user_input)
